# awesome-scalable-gnn and knowledge distillation
Papers about scalable Graph Neural Networks (GNNs) and Knowledge Distilition.
If you feel there are papers with related topics missing, do not hesitate to let us know (via issues or pull requests).

# Scalable-GNN
1. [Neurips 2017] **Inductive Representation Learning on Large Graphs** [[paper]](https://arxiv.org/abs/1706.02216)[[code]](https://github.com/williamleif/GraphSAGE)
2. [ICLR 2020] **GraphSAINT: Graph Sampling Based Inductive Learning Method** [[paper]](https://arxiv.org/abs/1907.04931)[[code]](https://github.com/GraphSAINT/GraphSAINT)

# Knowledge Distillation
1. [CVPR 2020] **Distilling Knowledge from Graph Convolutional Networks** [[paper]](https://arxiv.org/abs/2003.10477)[[code]](https://github.com/ihollywhy/DistillGCN.PyTorch)
2. [IJCAI 2021] **Graph-Free Knowledge Distillation for Graph Neural Networks** [[paper]](https://arxiv.org/pdf/2105.07519.pdf)[[code]](https://github.com/Xiang-Deng-DL/GFKD)
3. [International Journal of Computer Vision] **Knowledge Distillation: A Survey** [[paper]](https://arxiv.org/pdf/2006.05525.pdf)
